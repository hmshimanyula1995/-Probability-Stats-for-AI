{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b047b22-2a51-4d99-9ed9-bc0ed0533ee5",
   "metadata": {},
   "source": [
    "# CKD Risk Factors Prediction: Exploratory Data Analysis\n",
    "\n",
    "This repository contains the Exploratory Data Analysis (EDA) for the Chronic Kidney Disease (CKD) dataset. The goal is to predict CKD risk factors using Machine Learning techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5138c90-4fd6-4a46-93d8-9179b3ac87fc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The EDA involves data importing, data overview, data preprocessing, descriptive statistics, and data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc646a-3a9b-487f-b8de-b66c4bd36655",
   "metadata": {},
   "source": [
    "### Data Importing\n",
    "\n",
    "We started by importing the necessary libraries and loading the CKD dataset using the pandas read_csv function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9746dc5a-3642-4d48-a16a-0abb6679683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('ckd-dataset-v2 (2).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b561a37e-f971-4270-84ac-8ed9431d8842",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "We performed an initial exploration of the dataset by printing out the first few rows and checked the basic information of the dataset such as the number of entries, the data types of each column, and the presence of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703f0d8b-ac5f-42ad-8c8b-03b2652717bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows of the processed data\n",
    "print(df.head())\n",
    "\n",
    "# Print the basic information about the dataset\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea85183c-0c86-48fe-b4c5-13b093eeef39",
   "metadata": {},
   "source": [
    "## Data Preprocessing for EDA\n",
    "\n",
    "We found that two columns ('sg' and 'grf') contained a mixture of numeric ranges, discrete values, and greater than or equal to values. We created a function to handle these special cases and applied it to 'sg' and 'grf' columns to create new columns 'avg_sg' and 'avg_grf'. We then dropped the original 'sg' and 'grf' columns and converted 'class' column to binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ecf8ea-e020-41db-8808-b6efc46ace46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process 'sg' and 'grf' columns\n",
    "def process_column(col):\n",
    "    if isinstance(col, float):\n",
    "        if pd.isnull(col):\n",
    "            return np.nan\n",
    "    else:\n",
    "        if 'discrete' in col:\n",
    "            return np.nan\n",
    "        elif '-' in col:\n",
    "            return np.mean(list(map(float, col.split(' - '))))\n",
    "        elif 'â‰¥' in col:\n",
    "            return float(col[2:])\n",
    "        else:\n",
    "            try:\n",
    "                return float(col)\n",
    "            except:\n",
    "                return np.nan\n",
    "\n",
    "# Apply the function to 'sg' and 'grf' columns\n",
    "df['avg_sg'] = df['sg'].apply(process_column)\n",
    "df['avg_grf'] = df['grf'].apply(process_column)\n",
    "\n",
    "# Drop the original 'sg' and 'grf' columns\n",
    "df.drop(['sg', 'grf'], axis=1, inplace=True)\n",
    "\n",
    "# Convert 'class' column to binary format\n",
    "df['class'] = df['class'].map({'ckd': 1, 'notckd': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbbfc77-534b-4d85-99bf-103191f30ddf",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "We used the describe function to obtain descriptive statistics for the numeric columns in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8948865-0d24-4ff7-967c-4c3d7185103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the describe function\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cbd777-002e-40d1-ade2-0ea7bf6c48a7",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "We visualized the distribution of CKD and non-CKD patients using a bar plot. This helped us understand the balance of the target classes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895eb1e2-1366-4edd-82e4-83870f60fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for 'avg_sg' and 'avg_grf' columns\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['avg_sg'].dropna(), bins=30, kde=True)\n",
    "plt.title('avg_sg Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['avg_grf'].dropna(), bins=30, kde=True)\n",
    "plt.title('avg_grf Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='class', data=df)\n",
    "plt.title('CKD vs Non-CKD Patients')\n",
    "plt.xlabel('Groups')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.xticks([0, 1], ['Non-CKD', 'CKD'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c277a0f-813d-4785-bcf6-a1deba42d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Test sets\n",
    "We then split the data into training and test sets and visualized the distribution of CKD and non-CKD patients in both sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume 'class' is the target and rest of the columns are features\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to DataFrames for easier plotting\n",
    "y_train_df = pd.DataFrame(y_train, columns=['class'])\n",
    "y_test_df = pd.DataFrame(y_test, columns=['class'])\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Count plot for 'class' column in the training set\n",
    "sns.countplot(x='class', data=y_train_df, ax=axs[0])\n",
    "axs[0].set_title('CKD vs Non-CKD Patients (Training Set)')\n",
    "axs[0].set_xlabel('Groups')\n",
    "axs[0].set_ylabel('Number of Patients')\n",
    "axs[0].set_xticklabels(['Non-CKD', 'CKD'])\n",
    "\n",
    "# Count plot for 'class' column in the test set\n",
    "sns.countplot(x='class', data=y_test_df, ax=axs[1])\n",
    "axs[1].set_title('CKD vs Non-CKD Patients (Test Set)')\n",
    "axs[1].set_xlabel('Groups')\n",
    "axs[1].set_ylabel('Number of Patients')\n",
    "axs[1].set_xticklabels(['Non-CKD', 'CKD'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecae0e6-81c9-4acb-a86b-5473ba5d1ce7",
   "metadata": {},
   "source": [
    "## Data Preparation and Decision Stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd73ccbb-c853-4faf-9431-533f4115200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Upload file\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381d82b-94ef-4af7-8993-d8cca418442b",
   "metadata": {},
   "source": [
    "Steps to take:\n",
    "\n",
    "Importing Libraries: The necessary libraries and modules are imported. These include numpy, pandas, train_test_split from sklearn.model_selection, LogisticRegression from sklearn.linear_model, confusion_matrix, accuracy_score from sklearn.metrics, SimpleImputer from sklearn.impute, and OneHotEncoder from sklearn.preprocessing.\n",
    "\n",
    "Defining a Function: The function process_column is defined to handle the data preprocessing step. This function checks if a value in a column is 'discrete' or contains a '-', or is a float. It returns NaN for 'discrete', the average of the two numbers if the value contains a '-', and the float value if it's a float. If none of these conditions are met, it returns NaN.\n",
    "\n",
    "Loading the Dataset: The dataset is loaded from a CSV file using pd.read_csv.\n",
    "\n",
    "Data Preprocessing: The process_column function is applied to the necessary columns of the dataframe. The target column 'class' is converted to integer type, where 'ckd' is represented as 1 and 'notckd' as 0. Missing values in the dataframe are filled with the mean of the respective column. Then, categorical variables are one-hot encoded, and the original categorical columns are dropped from the dataframe.\n",
    "\n",
    "Splitting the Dataset: The dataset is split into features (X) and target (y). Then, it's further split into training and testing sets using train_test_split function.\n",
    "\n",
    "Data Imputation: A SimpleImputer object is created to fill any remaining missing values in the dataset with the mean of the respective column. This imputer is fit on the training data and then used to transform both training and testing data.\n",
    "\n",
    "Training the Model: A Logistic Regression model is trained using the imputed training data.\n",
    "\n",
    "Making Predictions: The model is used to make predictions on the test data.\n",
    "\n",
    "Evaluating the Model: The accuracy of the model is printed out, and a confusion matrix is displayed to evaluate the performance of the model.\n",
    "\n",
    "Note: This code is quite comprehensive and incorporates several good practices like handling missing values, converting data types, one-hot encoding categorical variables, and splitting the dataset into training and testing sets. It also makes use of logistic regression, a simple and commonly used machine learning algorithm for binary classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e44eeb-b9ce-446a-bfd6-1dff908b413e",
   "metadata": {},
   "source": [
    "**Hypertension (htn)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60cb16a-2dfc-4084-80c8-aea9b9619a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data into a pandas DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assume that 'df' is your DataFrame, 'htn' is the feature column, and 'classification' is the target column\n",
    "# Also assuming that 'classification' is a binary variable\n",
    "\n",
    "# Handle missing values in 'htn' column by filling with the mode\n",
    "df['htn'] = df['htn'].fillna(df['htn'].mode()[0])\n",
    "\n",
    "# Encode 'htn' column to numerical values if it's categorical\n",
    "le = LabelEncoder()\n",
    "df['htn'] = le.fit_transform(df['htn'])\n",
    "\n",
    "X = df[['htn']]  # feature\n",
    "y = df['class']  # target\n",
    "\n",
    "# Split the dataset into 70% training data and 30% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create an instance of DecisionTreeClassifier with max_depth = 1 (Decision Stump)\n",
    "clf = DecisionTreeClassifier(max_depth=1, random_state=0)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a78325-f80e-476d-9ba1-84a20ec88db7",
   "metadata": {},
   "source": [
    "**Diabetes Mellitus (dm)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ace0d1-fbd3-4657-b76d-3e905202b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data into a pandas DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assume that 'df' is your DataFrame, 'dm' is the feature column, and 'classification' is the target column\n",
    "# Also assuming that 'classification' is a binary variable\n",
    "\n",
    "# Handle missing values in 'dm' column by filling with the mode\n",
    "df['dm'] = df['dm'].fillna(df['dm'].mode()[0])\n",
    "\n",
    "# Encode 'dm' column to numerical values if it's categorical\n",
    "le = LabelEncoder()\n",
    "df['dm'] = le.fit_transform(df['dm'])\n",
    "\n",
    "X = df[['dm']]  # feature\n",
    "y = df['class']  # target\n",
    "\n",
    "# Split the dataset into 70% training data and 30% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create an instance of DecisionTreeClassifier with max_depth = 1 (Decision Stump)\n",
    "clf = DecisionTreeClassifier(max_depth=1, random_state=0)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248cf98-16f8-4c9c-a9ce-a31b549bf42a",
   "metadata": {},
   "source": [
    "**Hemoglobin A1c (hemo)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fcb723-73e6-4fd0-ae11-ff08bd56b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def process_column(col):\n",
    "    if 'discrete' in str(col):\n",
    "        return np.nan  # return NaN if 'discrete' is in column\n",
    "    if '-' in str(col):\n",
    "        low, high = map(float, str(col).split('-'))  # split on '-', convert to float\n",
    "        return (low + high) / 2  # return the average\n",
    "    else:\n",
    "        try:\n",
    "            return float(col)  # convert to float\n",
    "        except ValueError:\n",
    "            return np.nan  # if conversion to float fails, return NaN\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('ckd-dataset-v2 (2).csv')\n",
    "\n",
    "# Apply process_column function to necessary columns\n",
    "df['hemo'] = df['hemo'].apply(process_column)\n",
    "\n",
    "# Convert 'class' to integer type\n",
    "df['class'] = (df['class'] == 'ckd').astype(int)\n",
    "\n",
    "# Fill missing values with the mean of the respective column\n",
    "df['hemo'] = df['hemo'].fillna(df['hemo'].mean())\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = df[['hemo']]  # Select only the 'hemo' column as the feature\n",
    "y = df['class']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit on the training data\n",
    "imputer.fit(X_train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Train the model using the imputed training data\n",
    "model = DecisionTreeClassifier(max_depth=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print out the accuracy and confusion matrix\n",
    "print(f\"Accuracy: {round(accuracy_score(y_test, y_pred), 2)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818eb52-c13f-4f5b-8fd4-779510623e77",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a5cdc-b461-48fa-bdf8-10f81ac4eb2f",
   "metadata": {},
   "source": [
    "The Logistic Regression model from the sklearn.linear_model module.\n",
    "\n",
    "Here's a step-by-step breakdown of what the code is doing:\n",
    "\n",
    "StandardScaler(): This creates an instance of the StandardScaler class, which will be used to standardize the features by removing the mean and scaling to unit variance. This is often a good preprocessing step for many machine learning algorithms.\n",
    "\n",
    "scaler.fit_transform(X_train): This fits the scaler to the training data and then transforms the training data. \"Fitting\" the scaler means that it learns the parameters (mean and standard deviation for standardization) of the training data.\n",
    "\n",
    "scaler.transform(X_test): This uses the scaler that was fitted to the training data to transform the test data. It's important to note that the same scaler is used to transform both the training and test data to ensure that they are scaled in the same way.\n",
    "\n",
    "LogisticRegression(max_iter=1000): This creates an instance of the LogisticRegression class. The max_iter=1000 argument sets the maximum number of iterations for the solver to converge, which can be necessary for larger datasets.\n",
    "\n",
    "model.fit(X_train, y_train): This fits the logistic regression model to the training data. \"Fitting\" the model means that it learns the relationship between the features (X_train) and the target (y_train).\n",
    "\n",
    "model.predict(X_test): This uses the fitted model to make predictions on the test data.\n",
    "\n",
    "accuracy_score(y_test, y_pred): This calculates the accuracy of the model by comparing the predicted values to the actual values.\n",
    "\n",
    "So, in summary, this code is using logistic regression to make predictions on the test data and then calculating the accuracy of those predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fba00-3dc4-40dc-81af-d3092da5c4f5",
   "metadata": {},
   "source": [
    "**Hypertension (htn)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9514676-c829-4b9d-aa23-923a7183f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data into a pandas DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assume that 'df' is your DataFrame, 'htn' is the feature column, and 'classification' is the target column\n",
    "# Also assuming that 'classification' is a binary variable\n",
    "\n",
    "# Handle missing values in 'htn' column by filling with the mode\n",
    "df['htn'] = df['htn'].fillna(df['htn'].mode()[0])\n",
    "\n",
    "# Encode 'htn' column to numerical values if it's categorical\n",
    "le = LabelEncoder()\n",
    "df['htn'] = le.fit_transform(df['htn'])\n",
    "\n",
    "X = df[['htn']]  # feature\n",
    "y = df['class']  # target\n",
    "\n",
    "# Split the dataset into 70% training data and 30% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create an instance of LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2cd64f-4cfc-4fca-b7fc-f43aecac8fdf",
   "metadata": {},
   "source": [
    "**Hemoglobin A1c (hemo)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b36e8-ed8d-4f90-ae46-1e61cd258d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data into a pandas DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assume that 'df' is your DataFrame, 'hemo' is the feature column, and 'classification' is the target column\n",
    "# Also assuming that 'classification' is a binary variable\n",
    "\n",
    "# Handle missing values in 'hemo' column\n",
    "df['hemo'] = df['hemo'].fillna(df['hemo'].mean())\n",
    "\n",
    "X = df[['hemo']]  # feature\n",
    "y = df['class']  # target\n",
    "\n",
    "# Split the dataset into 70% training data and 30% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create an instance of LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be762459-dba4-4c75-86b8-7a35f4d69e67",
   "metadata": {},
   "source": [
    "**Diabetes Mellitus (dm)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0162c-1afd-4224-8f02-f330dbe454c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Assuming 'dm' is your feature and 'classification' is your target\n",
    "X = df['dm'].values.reshape(-1,1)\n",
    "y = df['class']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Creating the Logistic Regression model\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "\n",
    "# Training the model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \\n\", cm)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0edfc2-8526-4d6d-804b-ab36e193b286",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fb1bf-d9a9-49e7-bb18-191489786979",
   "metadata": {},
   "source": [
    "This script is about the Random Forest algorithm to a dataset for classification purposes, using the RandomForestClassifier class from the sklearn.ensemble module. Here are the key steps:\n",
    "\n",
    "RandomForestClassifier(n_estimators=100, random_state=42): This creates an instance of the RandomForestClassifier class with 100 trees in the forest (n_estimators=100) and a specified random state for reproducibility (random_state=42).\n",
    "\n",
    "rf.fit(X_train, y_train): This fits the Random Forest model to the training data. The model learns the relationship between the features (X_train) and the target (y_train) based on an ensemble of decision trees.\n",
    "\n",
    "rf.predict(X_test): This uses the fitted model to make predictions on the test data.\n",
    "\n",
    "accuracy_score(y_test, y_pred_rf): This computes the accuracy of the model by comparing the predicted values to the actual values.\n",
    "\n",
    "The Random Forest algorithm is a type of ensemble learning method, where multiple learning algorithms are used to obtain better predictive performance. In the case of Random Forest, it builds multiple decision trees and merges them together to get a more accurate and stable prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dcd51c-5c22-44e9-904e-0dd074d40356",
   "metadata": {},
   "source": [
    "**Hemoglobin A1c (hemo)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab8c0c-e71c-4e98-8130-e301decf852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data into a pandas DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assume that 'df' is your DataFrame, 'hemo' is the feature column, and 'classification' is the target column\n",
    "\n",
    "# Handle missing values in 'hemo' column by filling with the mean\n",
    "df['hemo'] = df['hemo'].fillna(df['hemo'].mean())\n",
    "\n",
    "X = df[['hemo']]  # feature\n",
    "y = df['class']  # target\n",
    "\n",
    "# Split the dataset into 70% training data and 30% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create an instance of RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b497c30-dcc3-4eb1-a181-9ca574c495de",
   "metadata": {},
   "source": [
    "**Hypertension (htn)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270aab6-a7e1-45b6-bf70-8ff85315185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data into a pandas DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Assume that 'df' is your DataFrame, 'htn' is the feature column, and 'classification' is the target column\n",
    "# Also assuming that 'classification' is a binary variable\n",
    "\n",
    "# Handle missing values in 'htn' column by filling with the mode\n",
    "df['htn'] = df['htn'].fillna(df['htn'].mode()[0])\n",
    "\n",
    "# Encode 'htn' column to numerical values if it's categorical\n",
    "le = LabelEncoder()\n",
    "df['htn'] = le.fit_transform(df['htn'])\n",
    "\n",
    "X = df[['htn']]  # feature\n",
    "y = df['class']  # target\n",
    "\n",
    "# Split the dataset into 70% training data and 30% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create an instance of RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b983a1-b813-4fa2-b50a-7d29d0eb8b92",
   "metadata": {},
   "source": [
    "**Diabetes Mellitus (dm)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa6a40d-fab9-40f7-b6c8-1d524a400e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Handling NaN values in 'dm' column\n",
    "df['dm'] = df['dm'].fillna(df['dm'].mean())\n",
    "\n",
    "# Selecting 'dm' as the feature and 'class' as the target\n",
    "X = df['dm'].values.reshape(-1,1)\n",
    "y = df['class'].values\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Initializing the Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# Training the model\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the test set results\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Printing the confusion matrix and accuracy score\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a95a33-e5b2-4131-8995-cabf79474fd9",
   "metadata": {},
   "source": [
    "## CKD Data Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e165f218-0e98-468b-a029-e8469128c797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[13  0]\n",
      " [ 0 28]]\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier  # Import DecisionTreeClassifier instead of LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def process_column(col):\n",
    "    if 'discrete' in str(col):\n",
    "        return np.nan  # return NaN if 'discrete' is in column\n",
    "    if '-' in str(col):\n",
    "        low, high = map(float, str(col).split('-'))  # split on '-', convert to float\n",
    "        return (low + high) / 2  # return the average\n",
    "    else:\n",
    "        try:\n",
    "            return float(col)  # convert to float\n",
    "        except ValueError:\n",
    "            return np.nan  # if conversion to float fails, return NaN\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('ckd-dataset-v2 (2).csv')\n",
    "\n",
    "# Added Affected - SR\n",
    "# Apply process_column function to necessary columns\n",
    "column_list = ['bp (Diastolic)', 'bp limit', 'sg', 'al', 'rbc', 'su', 'pc', 'pcc', 'ba', 'bgr', 'bu', 'sod', 'sc', 'pot', 'hemo', 'pcv', 'rbcc', 'wbcc', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane', 'grf', 'stage', 'affected', 'age']\n",
    "for column_name in column_list:\n",
    "    df[column_name] = df[column_name].apply(process_column)\n",
    "\n",
    "# Convert 'class' to integer type\n",
    "df['class'] = (df['class'] == 'ckd').astype(int)\n",
    "\n",
    "# Fill missing values with the mean of the respective column\n",
    "df = df.fillna(df.mean(numeric_only=True))\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "enc = OneHotEncoder(drop='first')  # Create encoder object\n",
    "df_encoded = pd.DataFrame(enc.fit_transform(df.select_dtypes(include=['object'])).toarray())  # Transform data\n",
    "\n",
    "# Merge with the original df\n",
    "df = df.join(df_encoded)\n",
    "df = df.drop(df.select_dtypes(include=['object']).columns, axis=1)\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert columns to string type to avoid issues with imputer\n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "# Use mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit on the training data\n",
    "imputer.fit(X_train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Train the model using the imputed training data\n",
    "model = DecisionTreeClassifier(max_depth=1)  # Replace LogisticRegression with DecisionTreeClassifier(max_depth=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Exclude rows by index values - SR\n",
    "filtered_df = df.drop([0, 1])\n",
    "\n",
    "\n",
    "# Print out the accuracy and confusion matrix\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830dd887-81cb-445d-a3d4-74265cad8811",
   "metadata": {},
   "source": [
    "## Affected column change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af474dea-09d8-4fab-8ada-c89d39b88b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bp (Diastolic)  bp limit     sg        al  class  rbc        su   pc  pcc  \\\n",
      "2             0.0       0.0  1.020  1.000000      1  0.0  2.724138  0.0  0.0   \n",
      "3             0.0       0.0  1.010  2.028169      1  0.0  2.724138  0.0  0.0   \n",
      "4             0.0       0.0  1.010  2.028169      1  1.0  2.724138  1.0  0.0   \n",
      "5             1.0       1.0  1.010  3.000000      1  0.0  2.724138  0.0  0.0   \n",
      "6             0.0       0.0  1.016  2.028169      1  0.0  2.724138  0.0  0.0   \n",
      "\n",
      "    ba  ...  htn   dm  cad  appet   pe  ane         grf  stage  affected  \\\n",
      "2  0.0  ...  0.0  0.0  0.0    0.0  0.0  0.0   90.897524    NaN       1.0   \n",
      "3  0.0  ...  0.0  0.0  0.0    0.0  0.0  0.0   90.897524    NaN       1.0   \n",
      "4  1.0  ...  0.0  0.0  0.0    1.0  0.0  0.0  139.863500    NaN       1.0   \n",
      "5  0.0  ...  0.0  0.0  0.0    0.0  0.0  0.0  139.863500    NaN       1.0   \n",
      "6  0.0  ...  0.0  1.0  0.0    1.0  1.0  0.0  139.863500    NaN       1.0   \n",
      "\n",
      "         age  \n",
      "2  52.973118  \n",
      "3  52.973118  \n",
      "4  52.973118  \n",
      "5  52.973118  \n",
      "6  16.000000  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Exclude rows by index values, removes descrete, blanks, and class\n",
    "filtered_df = df.drop([0, 1])\n",
    "\n",
    "# Assuming you have your dataset stored in a DataFrame called 'df'\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bf721c-bb49-4d3f-8847-93714a81967d",
   "metadata": {},
   "source": [
    "## Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d87954db-799b-47ba-8a51-8d2ff9d90b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               affected   R-squared:                       0.601\n",
      "Model:                            OLS   Adj. R-squared:                  0.595\n",
      "Method:                 Least Squares   F-statistic:                     98.35\n",
      "Date:                Mon, 26 Jun 2023   Prob (F-statistic):           7.08e-39\n",
      "Time:                        20:22:35   Log-Likelihood:                -45.148\n",
      "No. Observations:                 200   AIC:                             98.30\n",
      "Df Residuals:                     196   BIC:                             111.5\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         30.4514      6.044      5.039      0.000      18.533      42.370\n",
      "hemo          -0.1190      0.010    -11.699      0.000      -0.139      -0.099\n",
      "sg           -27.6260      5.974     -4.624      0.000     -39.408     -15.844\n",
      "grf           -0.0034      0.001     -6.150      0.000      -0.005      -0.002\n",
      "==============================================================================\n",
      "Omnibus:                        2.119   Durbin-Watson:                   1.879\n",
      "Prob(Omnibus):                  0.347   Jarque-Bera (JB):                1.718\n",
      "Skew:                          -0.067   Prob(JB):                        0.424\n",
      "Kurtosis:                       2.566   Cond. No.                     3.93e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.93e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming you have your data stored in X (independent variables) and y (dependent variable)\n",
    "X = filtered_df[['hemo','sg','grf']]\n",
    "y = filtered_df[['affected']]\n",
    "\n",
    "# Add a constant term to the independent variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model = sm.OLS(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "# Print the summary of the linear regression model\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770cde9f-38a6-41f3-93f5-d6c37406ec84",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We hope this analysis will contribute to a deeper understanding of the CKD dataset and aid in the development of effective Machine Learning models for CKD prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a532e2-2709-4027-aa00-50b558b5fc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
